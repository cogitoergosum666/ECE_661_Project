{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9acfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "text: in my palm is a clear stone , and inside it is a small ivory statuette . a guardian angel . `` figured if you 're going to be out at night getting hit by cars , you might as well have some backup . '' i look at him , feeling stunned . like this is some sort of sign . but as i stare at harlin , his mouth curved in a confident grin , i do n't care about signs\n",
      "domain: None\n",
      "\n",
      "Sample 2:\n",
      "text: give me a minute to change and i 'll meet you at the docks . '' she 'd forced those words through her teeth . `` no need to change . we wo n't be that long . '' shane gripped her arm and started leading her to the dock . `` i can make it there on my own , shane\n",
      "domain: None\n",
      "\n",
      "Sample 3:\n",
      "text: `` only one source i know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams , '' farrell concluded dryly . `` what can i say ? '' ellis unfolded his arms and widened his hands . `` your tax dollars at work . '' before farrell could respond , leila 's voice rose from inside the house . `` no insurance ? '' she wailed . `` what do you mean you do n't have any insurance\n",
      "domain: None\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cimec/lambada\", split=\"test\")\n",
    "\n",
    "# ÊâìÂç∞Ââç 3 Êù°ËÆ∞ÂΩï\n",
    "for i in range(3):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    for k, v in dataset[i].items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fcfdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:08:06,404\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 19:08:06 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === ÈÄöÁî®ËØÑ‰º∞ÂáΩÊï∞ ===\n",
    "def evaluate_lambada(model, tokenizer, dataset, name=\"Model\", max_tokens=1):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    wrong_cases = []\n",
    "\n",
    "    for example in tqdm(dataset, desc=f\"Evaluating {name}\"):\n",
    "        full_text = example[\"text\"].strip()\n",
    "        if not full_text or \" \" not in full_text:\n",
    "            continue  # ÂøΩÁï•ÂºÇÂ∏∏Ê†∑Êú¨\n",
    "\n",
    "        prompt = full_text.rsplit(\" \", 1)[0].strip() + \" \"\n",
    "        target = full_text.rsplit(\" \", 1)[1].strip()\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted = generated_text[len(prompt):].strip().split(\" \")[0]  # ÊúÄÂÖàÁîüÊàêÁöÑ token\n",
    "        if predicted.lower() == target.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            if len(wrong_cases) < 10:\n",
    "                wrong_cases.append((prompt, predicted, target))\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\n‚úÖ {name} Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(\"\\n‚ùå Example wrong predictions:\")\n",
    "    for p, pred, tgt in wrong_cases:\n",
    "        print(f\"Prompt: {p}\\n‚Üí Predicted: {pred} | Target: {tgt}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# === Âä†ËΩΩ LAMBADA Êï∞ÊçÆÈõÜ ===\n",
    "dataset = load_dataset(\"cimec/lambada\", split=\"test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f564a",
   "metadata": {},
   "source": [
    "Unsloth: Will load grpo500_phi14b_model_500eps_newreward as a legacy tokenizer.\n",
    "Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n",
    "are not enabled or a bias term (like in Qwen) is used.\n",
    "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
    "are not enabled or a bias term (like in Qwen) is used.\n",
    "Unsloth 2025.3.19 patched 40 layers with 0 QKV layers, 0 O layers and 40 MLP layers.\n",
    "Evaluating GRPO Fine-tuned Model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5153/5153 [23:43<00:00,  3.62it/s]\n",
    "\n",
    "‚úÖ GRPO Fine-tuned Model Accuracy: 5.76% (297/5153)\n",
    "\n",
    "‚ùå Example wrong predictions:\n",
    "Prompt: in my palm is a clear stone , and inside it is a small ivory statuette . a guardian angel . `` figured if you 're going to be out at night getting hit by cars , you might as well have some backup . '' i look at him , feeling stunned . like this is some sort of sign . but as i stare at harlin , his mouth curved in a confident grin , i do n't care about \n",
    "‚Üí Predicted: rome | Target: signs\n",
    "\n",
    "Prompt: give me a minute to change and i 'll meet you at the docks . '' she 'd forced those words through her teeth . `` no need to change . we wo n't be that long . '' shane gripped her arm and started leading her to the dock . `` i can make it there on my own , \n",
    "‚Üí Predicted: rome | Target: shane\n",
    "\n",
    "Prompt: helen 's heart broke a little in the face of miss mabel 's selfless courage . she thought that because she was old , her life was of less value than the others ' . for all helen knew , miss mabel had a lot more years to live than she did . `` not going to happen , '' replied \n",
    "‚Üí Predicted: ellen | Target: helen\n",
    "\n",
    "Prompt: preston had been the last person to wear those chains , and i knew what i 'd see and feel if they were slipped onto my skin-the reaper 's unending hatred of me . i 'd felt enough of that emotion already in the amphitheater . i did n't want to feel anymore . `` do n't put those on me , '' i whispered . `` please . '' sergei looked at me , surprised by my low , raspy please , but he put down the \n",
    "‚Üí Predicted: urch | Target: chains\n",
    "\n",
    "Prompt: she knew that basha was a decent young man , that he was pretty sweet and friendly with her . jawen knew they had a bit of a history , but she thought that this time she would get along better with him , that she could overlook those problems . they kissed , and she knew that she liked basha , but then hastin interfered . she was so angry that she immediately said , once they were out of earshot of basha , `` you do n't mean anything to me anymore , \n",
    "‚Üí Predicted: 0 | Target: hastin\n",
    "\n",
    "Prompt: he heard rhinna speak `` the queen wants you in her carriage . '' tom spoke `` no , i 'm not going in some asylum . '' ran was seen standing next to him spoke `` it 's just for a private talk with you that 's all . '' tom groaned and went inside the carriage to sit down next to the \n",
    "‚Üí Predicted: 2 | Target: queen\n",
    "\n",
    "Prompt: there was no way he would come here on his own . he ordered a cup of coffee , and then we just sat in silence . `` so , '' aidan finally said , `` how 's it going ? '' i laughed . `` not much has changed since the last time i saw you . '' `` ya know , you eat here a lot , '' said \n",
    "‚Üí Predicted: ellen | Target: aidan\n",
    "\n",
    "Prompt: `` why ? '' `` i would have thought you 'd find him rather dry , '' she said . `` i do n't know about that , '' said gabriel . `` he was a great craftsman , '' said heather . `` that he was , '' said flannery . `` and polish , to boot , '' said \n",
    "‚Üí Predicted: ellen | Target: gabriel\n",
    "\n",
    "Prompt: both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen , and i was glad to relax against the tree 's rough , brittle bark and begin my breakfast of buttery , toasted bread and fresh fruit . even the water was tasty , it was so clean and cold . it almost made up for the lack of \n",
    "‚Üí Predicted: iced | Target: coffee\n",
    "\n",
    "Prompt: escorting drunk humans out of the bar is different from going up against a tiger-wildcat who eats raw steak for breakfast and is dying for a fight . '' `` i bet he could win with just his breath , '' ronan said . sean chuckled . `` take it seriously , ronan . these guys are seasoned . if marquez has a champion , it means he 's won a good share of the \n",
    "‚Üí Predicted: 100 | Target: fights\n",
    "\n",
    "Unsloth: Switching from Unsloth dynamic quant to normal quant since\n",
    "we do not yet support fast inference for unsloth/phi-4-unsloth-bnb-4bit\n",
    "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
    "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.996 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
    "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
    "Unsloth: vLLM loading unsloth/phi-4-bnb-4bit with actual GPU utilization = 12.81%\n",
    "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 16.0 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 128.\n",
    "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 2 GB.\n",
    "INFO 04-19 10:24:25 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
    "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
    "INFO 04-19 10:24:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/phi-4-bnb-4bit', speculative_config=None, tokenizer='unsloth/phi-4-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/phi-4-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
    "INFO 04-19 10:24:26 model_runner.py:1110] Starting to load model unsloth/phi-4-bnb-4bit...\n",
    "Unsloth: Retrying vLLM to process 96 sequences and 256 tokens in tandem.\n",
    "Error:\n",
    "CUDA driver error: out of memory\n",
    "INFO 04-19 10:24:28 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
    "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
    "INFO 04-19 10:24:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/phi-4-bnb-4bit', speculative_config=None, tokenizer='unsloth/phi-4-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/phi-4-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":96}, use_cached_outputs=False, \n",
    "INFO 04-19 10:24:28 model_runner.py:1110] Starting to load model unsloth/phi-4-bnb-4bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503b99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Âä†ËΩΩ‰Ω†ËÆ≠ÁªÉÂ•ΩÁöÑ GRPO LoRA Ê®°Âûã ===\n",
    "# finetuned_model_path = \"grpo500_phi14b_model_500eps_newreward\"\n",
    "# model_finetuned, tokenizer_finetuned = FastLanguageModel.from_pretrained(\n",
    "#     model_name=finetuned_model_path,\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=True,\n",
    "#     fast_inference=True,\n",
    "#     max_lora_rank=64,\n",
    "#     gpu_memory_utilization=0.7,\n",
    "#     # trust_remote_code=True\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model_finetuned)\n",
    "\n",
    "# === ËØÑ‰º∞ÂæÆË∞ÉÊ®°Âûã ===\n",
    "# evaluate_lambada(model_finetuned, tokenizer_finetuned, dataset, name=\"GRPO Fine-tuned Model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e4bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Âä†ËΩΩÂéüÂßã base Ê®°Âûã unsloth/Phi-4 ===\n",
    "# model_base, tokenizer_base = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"unsloth/Phi-4\",\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=True,\n",
    "#     fast_inference=True,\n",
    "#     max_lora_rank=64,\n",
    "#     gpu_memory_utilization=0.7,\n",
    " \n",
    "# )\n",
    "# FastLanguageModel.for_inference(model_base)\n",
    "\n",
    "# # === ËØÑ‰º∞ base Ê®°Âûã ===\n",
    "# evaluate_lambada(model_base, tokenizer_base, dataset, name=\"Base Model (unsloth/Phi-4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec2d88",
   "metadata": {},
   "source": [
    "‚úÖ Base Model (unsloth/Phi-4) Accuracy: 5.84% (301/5153)\n",
    "\n",
    "‚ùå Example wrong predictions:\n",
    "Prompt: in my palm is a clear stone , and inside it is a small ivory statuette . a guardian angel . `` figured if you 're going to be out at night getting hit by cars , you might as well have some backup . '' i look at him , feeling stunned . like this is some sort of sign . but as i stare at harlin , his mouth curved in a confident grin , i do n't care about \n",
    "‚Üí Predicted: rome | Target: signs\n",
    "\n",
    "Prompt: give me a minute to change and i 'll meet you at the docks . '' she 'd forced those words through her teeth . `` no need to change . we wo n't be that long . '' shane gripped her arm and started leading her to the dock . `` i can make it there on my own , \n",
    "‚Üí Predicted: elli | Target: shane\n",
    "\n",
    "Prompt: helen 's heart broke a little in the face of miss mabel 's selfless courage . she thought that because she was old , her life was of less value than the others ' . for all helen knew , miss mabel had a lot more years to live than she did . `` not going to happen , '' replied \n",
    "‚Üí Predicted: ellen | Target: helen\n",
    "\n",
    "Prompt: preston had been the last person to wear those chains , and i knew what i 'd see and feel if they were slipped onto my skin-the reaper 's unending hatred of me . i 'd felt enough of that emotion already in the amphitheater . i did n't want to feel anymore . `` do n't put those on me , '' i whispered . `` please . '' sergei looked at me , surprised by my low , raspy please , but he put down the \n",
    "‚Üí Predicted: urch | Target: chains\n",
    "\n",
    "Prompt: she knew that basha was a decent young man , that he was pretty sweet and friendly with her . jawen knew they had a bit of a history , but she thought that this time she would get along better with him , that she could overlook those problems . they kissed , and she knew that she liked basha , but then hastin interfered . she was so angry that she immediately said , once they were out of earshot of basha , `` you do n't mean anything to me anymore , \n",
    "‚Üí Predicted: 0 | Target: hastin\n",
    "\n",
    "Prompt: he heard rhinna speak `` the queen wants you in her carriage . '' tom spoke `` no , i 'm not going in some asylum . '' ran was seen standing next to him spoke `` it 's just for a private talk with you that 's all . '' tom groaned and went inside the carriage to sit down next to the \n",
    "‚Üí Predicted: 2 | Target: queen\n",
    "\n",
    "Prompt: there was no way he would come here on his own . he ordered a cup of coffee , and then we just sat in silence . `` so , '' aidan finally said , `` how 's it going ? '' i laughed . `` not much has changed since the last time i saw you . '' `` ya know , you eat here a lot , '' said \n",
    "‚Üí Predicted: ellen | Target: aidan\n",
    "\n",
    "Prompt: `` why ? '' `` i would have thought you 'd find him rather dry , '' she said . `` i do n't know about that , '' said gabriel . `` he was a great craftsman , '' said heather . `` that he was , '' said flannery . `` and polish , to boot , '' said \n",
    "‚Üí Predicted: ellen | Target: gabriel\n",
    "\n",
    "Prompt: both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen , and i was glad to relax against the tree 's rough , brittle bark and begin my breakfast of buttery , toasted bread and fresh fruit . even the water was tasty , it was so clean and cold . it almost made up for the lack of \n",
    "‚Üí Predicted: iced | Target: coffee\n",
    "\n",
    "Prompt: escorting drunk humans out of the bar is different from going up against a tiger-wildcat who eats raw steak for breakfast and is dying for a fight . '' `` i bet he could win with just his breath , '' ronan said . sean chuckled . `` take it seriously , ronan . these guys are seasoned . if marquez has a champion , it means he 's won a good share of the \n",
    "‚Üí Predicted: 100 | Target: fights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25162e74",
   "metadata": {},
   "source": [
    "# Âä†‰∫Üprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa999124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Âä†ËΩΩËØ≠‰πâÂµåÂÖ•Ê®°ÂûãÔºàÂª∫ËÆÆ all-MiniLM-L6-v2ÔºåËΩªÈáèÁ®≥ÂÆöÔºâ\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def is_semantically_similar(pred, target, threshold=0.63):\n",
    "    pred_sent = f\"The final word is {pred}.\"\n",
    "    tgt_sent = f\"The final word is {target}.\"\n",
    "    embeddings = embedder.encode([pred_sent, tgt_sent], convert_to_tensor=True)\n",
    "    score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "    return score >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48269f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import string\n",
    "\n",
    "# ‰Ω†ËÆ≠ÁªÉÊó∂Áî®ÁöÑ system prompt\n",
    "SYSTEM_PROMPT = \"\"\"Respond in the following format:\n",
    "<reasoning>\n",
    "Your reasoning here...\n",
    "</reasoning>\n",
    "<answer>\n",
    "The final word (only one word, no explanation)\n",
    "</answer>\"\"\"\n",
    "\n",
    "def apply_chat_prompt(tokenizer, context_text: str) -> str:\n",
    "    return tokenizer.apply_chat_template([\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Please read the following text and predict its final word:\\n\\n{context_text}\"}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def extract_answer_from_xml(output: str) -> str:\n",
    "    matches = re.findall(r\"<answer>\\s*(.*?)\\s*</answer>\", output, re.DOTALL)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    \n",
    "    # ÂèñÊúÄÂêé‰∏Ä‰∏™ <answer>ÔºåÂéªÈô§Ê†áÁÇπÔºåÂè™‰øùÁïôÁ¨¨‰∏Ä‰∏™ÂçïËØç\n",
    "    raw = matches[-1].strip()\n",
    "    word = raw.split()[0].strip(string.punctuation)\n",
    "    \n",
    "    # ÂèØÈÄâÔºöËøáÊª§Â∏∏ËßÅ collapse token\n",
    "    if word.lower() in {\"the\", \"a\", \"an\", \"it\"}:\n",
    "        return \"\"\n",
    "    \n",
    "    return word\n",
    "\n",
    "def evaluate_lambada_structured(model, tokenizer, dataset, name=\"Model\"):\n",
    "    model.eval()\n",
    "    strict_correct = 0\n",
    "    soft_correct = 0\n",
    "    total = 0\n",
    "    wrong_cases = []\n",
    "\n",
    "    for example in tqdm(dataset, desc=f\"Evaluating {name}\"):\n",
    "        full_text = example[\"text\"].strip()\n",
    "        if not full_text or \" \" not in full_text:\n",
    "            continue\n",
    "        prompt_raw = full_text.rsplit(\" \", 1)[0].strip()\n",
    "        target = full_text.rsplit(\" \", 1)[1].strip()\n",
    "\n",
    "        # ÊûÑÈÄ† prompt\n",
    "        chat_prompt = apply_chat_prompt(tokenizer, prompt_raw)\n",
    "        inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted = extract_answer_from_xml(generated_text)\n",
    "        total += 1\n",
    "\n",
    "        if predicted.lower() == target.lower():\n",
    "            strict_correct += 1\n",
    "        elif is_semantically_similar(predicted.lower(), target.lower()):\n",
    "            soft_correct += 1\n",
    "        else:\n",
    "            if len(wrong_cases) < 10:\n",
    "                wrong_cases.append((prompt_raw, predicted, target))\n",
    "\n",
    "    # ËæìÂá∫ËØÑ‰º∞ÁªìÊûú\n",
    "    print(f\"\\n‚úÖ {name} Strict Accuracy: {strict_correct / total:.2%} ({strict_correct}/{total})\")\n",
    "    print(f\"ü§ù {name} Semantic Accuracy: {(strict_correct + soft_correct) / total:.2%} \"\n",
    "          f\"({strict_correct + soft_correct}/{total})\")\n",
    "\n",
    "    print(\"\\n‚ùå Example wrong predictions:\")\n",
    "    for p, pred, tgt in wrong_cases:\n",
    "        print(f\"Context: {p}\\n‚Üí Predicted: {pred} | Target: {tgt}\\n\")\n",
    "\n",
    "# def evaluate_lambada_structured(model, tokenizer, dataset, name=\"Model\"):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     wrong_cases = []\n",
    "\n",
    "#     for example in tqdm(dataset, desc=f\"Evaluating {name}\"):\n",
    "#         full_text = example[\"text\"].strip()\n",
    "#         if not full_text or \" \" not in full_text:\n",
    "#             continue\n",
    "#         prompt_raw = full_text.rsplit(\" \", 1)[0].strip()\n",
    "#         target = full_text.rsplit(\" \", 1)[1].strip()\n",
    "\n",
    "#         # ÊûÑÈÄ† prompt\n",
    "#         chat_prompt = apply_chat_prompt(tokenizer, prompt_raw)\n",
    "#         inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs,\n",
    "#                 max_new_tokens=256,\n",
    "#                 do_sample=False,\n",
    "#                 temperature=0.0,\n",
    "#                 top_p=1.0,\n",
    "#                 pad_token_id=tokenizer.eos_token_id,\n",
    "#             )\n",
    "\n",
    "#         generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#         predicted = extract_answer_from_xml(generated_text)\n",
    "\n",
    "#         if predicted.lower() == target.lower():\n",
    "#             correct += 1\n",
    "#         else:\n",
    "#             if len(wrong_cases) < 10:\n",
    "#                 wrong_cases.append((prompt_raw, predicted, target))\n",
    "#         total += 1\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"\\n‚úÖ {name} Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "#     print(\"\\n‚ùå Example wrong predictions:\")\n",
    "#     for p, pred, tgt in wrong_cases:\n",
    "#         print(f\"Context: {p}\\n‚Üí Predicted: {pred} | Target: {tgt}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0127ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âä†ËΩΩ LAMBADA Êï∞ÊçÆÈõÜ\n",
    "#dataset = load_dataset(\"cimec/lambada\", split=\"test\")\n",
    "dataset = load_dataset(\"cimec/lambada\", split=\"test[:5%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77cf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Âä†ËΩΩ GRPO ÂæÆË∞ÉÊ®°Âûã\n",
    "# model_finetuned, tokenizer_finetuned = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"grpo500_phi14b_model_500eps_newreward\",\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=True,\n",
    "#     fast_inference=True,\n",
    "#     max_lora_rank=64,\n",
    "#     gpu_memory_utilization=0.7,\n",
    "    \n",
    "# )\n",
    "# FastLanguageModel.for_inference(model_finetuned)\n",
    "\n",
    "# evaluate_lambada_structured(model_finetuned, tokenizer_finetuned, dataset, name=\"GRPO Fine-tuned Model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a1dbe",
   "metadata": {},
   "source": [
    "Âè™‰øÆÊîπrewardÂáΩÊï∞ÔºöÔºà25%È¢òÈáèÔºâ\n",
    "\n",
    "‚úÖ GRPO Fine-tuned Model Accuracy: 5.76% (297/5153)\n",
    "\n",
    "‚úÖ Base Model (unsloth/Phi-4) Accuracy: 5.84% (301/5153)\n",
    "\n",
    "ÊîπËøõÁ≠îÊ°àÊäìÂèñÂáΩÊï∞ÂêéÔºöÔºà5%È¢òÈáèÔºâ\n",
    "\n",
    "‚úÖ GRPO Fine-tuned Model Accuracy: 33.33% (86/258)\n",
    "\n",
    "‚úÖ Base Model (unsloth/Phi-4) Accuracy: 34.11% (88/258)\n",
    "\n",
    "Ê∑ªÂä†ËØ≠‰πâÁ≠îÊ°àÂåπÈÖçÂáΩÊï∞ÂêéÔºöÔºà3%È¢òÈáèÔºâ\n",
    "\n",
    "\n",
    "Ê∑ªÂä†ËØ≠‰πâÁ≠îÊ°àÂåπÈÖçÂáΩÊï∞ÂêéÔºöÔºà5%È¢òÈáèÔºâ\n",
    "\n",
    "‚úÖ GRPO Fine-tuned Model Strict Accuracy: 33.33% (86/258)\n",
    "ü§ù GRPO Fine-tuned Model Semantic Accuracy: 46.12% (119/258)\n",
    "\n",
    "‚úÖ Base Model (unsloth/Phi-4) Strict Accuracy: 34.11% (88/258)\n",
    "ü§ù Base Model (unsloth/Phi-4) Semantic Accuracy: 45.74% (118/258)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e84200",
   "metadata": {},
   "source": [
    "‚úÖ GRPO Fine-tuned Model Accuracy: 33.33% (86/258)\n",
    "\n",
    "‚ùå Example wrong predictions:\n",
    "Context: in my palm is a clear stone , and inside it is a small ivory statuette . a guardian angel . `` figured if you 're going to be out at night getting hit by cars , you might as well have some backup . '' i look at him , feeling stunned . like this is some sort of sign . but as i stare at harlin , his mouth curved in a confident grin , i do n't care about\n",
    "‚Üí Predicted: danger | Target: signs\n",
    "\n",
    "Context: give me a minute to change and i 'll meet you at the docks . '' she 'd forced those words through her teeth . `` no need to change . we wo n't be that long . '' shane gripped her arm and started leading her to the dock . `` i can make it there on my own ,\n",
    "‚Üí Predicted: alone | Target: shane\n",
    "\n",
    "Context: `` only one source i know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams , '' farrell concluded dryly . `` what can i say ? '' ellis unfolded his arms and widened his hands . `` your tax dollars at work . '' before farrell could respond , leila 's voice rose from inside the house . `` no insurance ? '' she wailed . `` what do you mean you do n't have any\n",
    "‚Üí Predicted: coverage | Target: insurance\n",
    "\n",
    "Context: helen 's heart broke a little in the face of miss mabel 's selfless courage . she thought that because she was old , her life was of less value than the others ' . for all helen knew , miss mabel had a lot more years to live than she did . `` not going to happen , '' replied\n",
    "‚Üí Predicted: that | Target: helen\n",
    "\n",
    "Context: there was no way he would come here on his own . he ordered a cup of coffee , and then we just sat in silence . `` so , '' aidan finally said , `` how 's it going ? '' i laughed . `` not much has changed since the last time i saw you . '' `` ya know , you eat here a lot , '' said\n",
    "‚Üí Predicted: regular | Target: aidan\n",
    "\n",
    "Context: `` why ? '' `` i would have thought you 'd find him rather dry , '' she said . `` i do n't know about that , '' said gabriel . `` he was a great craftsman , '' said heather . `` that he was , '' said flannery . `` and polish , to boot , '' said\n",
    "‚Üí Predicted: O'Connor | Target: gabriel\n",
    "\n",
    "Context: escorting drunk humans out of the bar is different from going up against a tiger-wildcat who eats raw steak for breakfast and is dying for a fight . '' `` i bet he could win with just his breath , '' ronan said . sean chuckled . `` take it seriously , ronan . these guys are seasoned . if marquez has a champion , it means he 's won a good share of the\n",
    "‚Üí Predicted: titles | Target: fights\n",
    "\n",
    "Context: i was so happy to see him that i almost sobbed his name . eli stiffened and let out a hiss . `` mohiri ! '' fear crept into his voice , and my dazed mind wondered what on earth scared a vampire . nikolas chuckled , and i felt a tremor run through my captor . `` i see there is no need for\n",
    "‚Üí Predicted: Mohiri | Target: introductions\n",
    "\n",
    "Context: `` come on , baby girl , '' mary jo said , scooping up the toy , then bending to retrieve her daughter . `` let me change your diaper and put you down for a couple of hours . '' `` she sleeps that long ? '' `` almost every afternoon . she still takes a morning nap , too , but she 'll outgrow those pretty soon . '' lori knew she had a lot to learn about\n",
    "‚Üí Predicted: parenting | Target: babies\n",
    "\n",
    "Context: i would really like to have some time with my mom . '' lucien groaned inwardly . how could he say no to that ? julia was grieving for her mate and at a loss on how to deal with her daughter . now jaeden actually wanted to speak to her mom ... he would be an ogre if he said no . he gave a quick nod and ignored the grateful smile she threw him before her and her mother ambled out of the room\n",
    "‚Üí Predicted: room | Target: together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f5e962",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_finetuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_finetuned\u001b[49m.print_trainable_parameters()\n",
      "\u001b[31mNameError\u001b[39m: name 'model_finetuned' is not defined"
     ]
    }
   ],
   "source": [
    "model_finetuned.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "import string\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Respond in the following format:\n",
    "<reasoning>\n",
    "Your reasoning here...\n",
    "</reasoning>\n",
    "<answer>\n",
    "The final word (only one word, no explanation)\n",
    "</answer>\"\"\"\n",
    "\n",
    "def apply_chat_prompt(tokenizer, context_text: str) -> str:\n",
    "    return tokenizer.apply_chat_template([\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Please read the following text and predict its final word:\\n\\n{context_text}\"}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def extract_answer_from_xml(output: str) -> str:\n",
    "    matches = re.findall(r\"<answer>\\s*(.*?)\\s*</answer>\", output, re.DOTALL)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    \n",
    "    # ÂèñÊúÄÂêé‰∏Ä‰∏™ <answer>ÔºåÂéªÈô§Ê†áÁÇπÔºåÂè™‰øùÁïôÁ¨¨‰∏Ä‰∏™ÂçïËØç\n",
    "    raw = matches[-1].strip()\n",
    "    word = raw.split()[0].strip(string.punctuation)\n",
    "    \n",
    "    # ÂèØÈÄâÔºöËøáÊª§Â∏∏ËßÅ collapse token\n",
    "    if word.lower() in {\"the\", \"a\", \"an\", \"it\"}:\n",
    "        return \"\"\n",
    "    \n",
    "    return word\n",
    "\n",
    "# def is_semantically_similar(pred, target, threshold=0.75):\n",
    "#     embeddings = embedder.encode([pred, target], convert_to_tensor=True)\n",
    "#     score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "#     print(f\"üîç Sim({pred}, {target}) = {score:.3f}\")\n",
    "#     return score >= threshold\n",
    "\n",
    "def is_semantically_similar(pred, target, threshold=0.7):\n",
    "    pred_sent = f\"The final word is {pred}.\"\n",
    "    tgt_sent = f\"The final word is {target}.\"\n",
    "    embeddings = embedder.encode([pred_sent, tgt_sent], convert_to_tensor=True)\n",
    "    score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "    print(f\"üîç Sim({pred}, {target}) = {score:.3f}\")\n",
    "    return score >= threshold\n",
    "\n",
    "\n",
    "\n",
    "def test_one_lambada_sample(model, tokenizer, example, max_new_tokens=256):\n",
    "    model.eval()\n",
    "\n",
    "    full_text = example[\"text\"].strip()\n",
    "    if not full_text or \" \" not in full_text:\n",
    "        print(\"‚ö†Ô∏è Invalid sample.\")\n",
    "        return\n",
    "\n",
    "    prompt_raw = full_text.rsplit(\" \", 1)[0].strip()\n",
    "    target = full_text.rsplit(\" \", 1)[1].strip()\n",
    "\n",
    "    # === ÊûÑÈÄ† ChatML prompt ===\n",
    "    chat_prompt = apply_chat_prompt(tokenizer, prompt_raw)\n",
    "    print(\"üîπ System + User Prompt:\\n\", chat_prompt)\n",
    "\n",
    "    # === Ê®°ÂûãÁîüÊàê ===\n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nüî∏ Raw Generated Output:\\n\", generated_text)\n",
    "\n",
    "    # === Ëß£Êûê <answer> ÈÉ®ÂàÜ ===\n",
    "    predicted = extract_answer_from_xml(generated_text)\n",
    "    print(\"\\n‚úÖ Extracted <answer>:\", predicted)\n",
    "    print(\"üéØ Target:\", target)\n",
    "    print(is_semantically_similar(predicted.lower(), target.lower(),0.6))\n",
    "\n",
    "    if predicted.lower() == target.lower():\n",
    "        print(\"\\nüéâ Prediction CORRECT ‚úÖ\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Prediction WRONG ‚ùå\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f79d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ System + User Prompt:\n",
      " <|im_start|>system<|im_sep|>Respond in the following format:\n",
      "<reasoning>\n",
      "Your reasoning here...\n",
      "</reasoning>\n",
      "<answer>\n",
      "The final word (only one word, no explanation)\n",
      "</answer><|im_end|><|im_start|>user<|im_sep|>Please read the following text and predict its final word:\n",
      "\n",
      "`` only one source i know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams , '' farrell concluded dryly . `` what can i say ? '' ellis unfolded his arms and widened his hands . `` your tax dollars at work . '' before farrell could respond , leila 's voice rose from inside the house . `` no insurance ? '' she wailed . `` what do you mean you do n't have any<|im_end|><|im_start|>assistant<|im_sep|>\n",
      "\n",
      "üî∏ Raw Generated Output:\n",
      " <|im_start|> system <|im_sep|> Respond in the following format:\n",
      "<reasoning>\n",
      "Your reasoning here...\n",
      "</reasoning>\n",
      "<answer>\n",
      "The final word (only one word, no explanation)\n",
      "</answer> <|im_start|> user <|im_sep|> Please read the following text and predict its final word:\n",
      "\n",
      "`` only one source i know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams , '' farrell concluded dryly . `` what can i say ? '' ellis unfolded his arms and widened his hands . `` your tax dollars at work . '' before farrell could respond , leila 's voice rose from inside the house . `` no insurance ? '' she wailed . `` what do you mean you do n't have any <|im_start|> assistant <|im_sep|> <reasoning>\n",
      "The text appears to be a dialogue from a narrative, likely a scene from a story or script. The characters are discussing a peculiar situation involving a \"phony sleep research facility\" and the use of dreams to solve crimes. The dialogue ends with Leila expressing concern about insurance, suggesting a potential problem or complication related to the facility or the activities being conducted there. The phrase \"what do you mean you don't have any\" implies that the next word would logically complete the sentence about insurance, likely indicating a type of insurance that is expected but missing. Common types of insurance in such contexts could include liability insurance, health insurance, or some form of coverage related to the activities being discussed.\n",
      "</reasoning>\n",
      "<answer>\n",
      "coverage\n",
      "</answer>## Question\n",
      "In a small town, six friends‚ÄîAlice, Bob, Charlie, Diana, Edward, and Fiona‚Äîeach have a unique favorite number between 1 and 6, a unique favorite color, and a unique favorite fruit. Use the following clues to determine each person's favorite number, color, and fruit:\n",
      "\n",
      "1. Alice's favorite number is twice Diana's favorite number.\n",
      "2. Bob's favorite number is one more than Fiona's favorite number.\n",
      "3. Charlie's favorite number is the highest\n",
      "\n",
      "‚úÖ Extracted <answer>: coverage\n",
      "üéØ Target: insurance\n",
      "üîç Sim(coverage, insurance) = 0.637\n",
      "True\n",
      "\n",
      "‚ùå Prediction WRONG ‚ùå\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Áî® GRPO ÂæÆË∞ÉÊ®°ÂûãÊµãËØïÁ¨¨ 3 ‰∏™Ê†∑Êú¨\n",
    "test_one_lambada_sample(model_finetuned, tokenizer_finetuned, dataset[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d309b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching from Unsloth dynamic quant to normal quant since\n",
      "we do not yet support fast inference for unsloth/phi-4-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
      "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
      "Unsloth: vLLM loading unsloth/phi-4-bnb-4bit with actual GPU utilization = 64.6%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 16.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 3 GB.\n",
      "INFO 04-19 19:08:23 config.py:549] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-19 19:08:23 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/phi-4-bnb-4bit', speculative_config=None, tokenizer='unsloth/phi-4-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/phi-4-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
      "WARNING 04-19 19:08:23 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 04-19 19:08:23 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-19 19:08:23 model_runner.py:1110] Starting to load model unsloth/phi-4-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W419 19:08:23.719867578 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 19:08:24 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 04-19 19:08:24 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 19:08:28 model_runner.py:1115] Loading model weights took 8.4920 GB\n",
      "INFO 04-19 19:08:28 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 19:08:29 worker.py:267] Memory profiling takes 1.18 seconds\n",
      "INFO 04-19 19:08:29 worker.py:267] the current vLLM instance can use total_gpu_memory (16.00GiB) x gpu_memory_utilization (0.65) = 10.33GiB\n",
      "INFO 04-19 19:08:29 worker.py:267] model weights take 8.49GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 1.34GiB.\n",
      "INFO 04-19 19:08:30 executor_base.py:111] # cuda blocks: 439, # CPU blocks: 983\n",
      "INFO 04-19 19:08:30 executor_base.py:116] Maximum concurrency for 256 tokens per request: 27.44x\n",
      "INFO 04-19 19:08:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:18<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 19:08:48 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.65 GiB\n",
      "INFO 04-19 19:08:48 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Base Model (unsloth/Phi-4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 258/258 [40:21<00:00,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Base Model (unsloth/Phi-4) Strict Accuracy: 34.11% (88/258)\n",
      "ü§ù Base Model (unsloth/Phi-4) Semantic Accuracy: 45.74% (118/258)\n",
      "\n",
      "‚ùå Example wrong predictions:\n",
      "Context: in my palm is a clear stone , and inside it is a small ivory statuette . a guardian angel . `` figured if you 're going to be out at night getting hit by cars , you might as well have some backup . '' i look at him , feeling stunned . like this is some sort of sign . but as i stare at harlin , his mouth curved in a confident grin , i do n't care about\n",
      "‚Üí Predicted: him | Target: signs\n",
      "\n",
      "Context: give me a minute to change and i 'll meet you at the docks . '' she 'd forced those words through her teeth . `` no need to change . we wo n't be that long . '' shane gripped her arm and started leading her to the dock . `` i can make it there on my own ,\n",
      "‚Üí Predicted: alone | Target: shane\n",
      "\n",
      "Context: helen 's heart broke a little in the face of miss mabel 's selfless courage . she thought that because she was old , her life was of less value than the others ' . for all helen knew , miss mabel had a lot more years to live than she did . `` not going to happen , '' replied\n",
      "‚Üí Predicted:  | Target: helen\n",
      "\n",
      "Context: there was no way he would come here on his own . he ordered a cup of coffee , and then we just sat in silence . `` so , '' aidan finally said , `` how 's it going ? '' i laughed . `` not much has changed since the last time i saw you . '' `` ya know , you eat here a lot , '' said\n",
      "‚Üí Predicted: I | Target: aidan\n",
      "\n",
      "Context: `` why ? '' `` i would have thought you 'd find him rather dry , '' she said . `` i do n't know about that , '' said gabriel . `` he was a great craftsman , '' said heather . `` that he was , '' said flannery . `` and polish , to boot , '' said\n",
      "‚Üí Predicted: said | Target: gabriel\n",
      "\n",
      "Context: both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen , and i was glad to relax against the tree 's rough , brittle bark and begin my breakfast of buttery , toasted bread and fresh fruit . even the water was tasty , it was so clean and cold . it almost made up for the lack of\n",
      "‚Üí Predicted: kitchen | Target: coffee\n",
      "\n",
      "Context: i was so happy to see him that i almost sobbed his name . eli stiffened and let out a hiss . `` mohiri ! '' fear crept into his voice , and my dazed mind wondered what on earth scared a vampire . nikolas chuckled , and i felt a tremor run through my captor . `` i see there is no need for\n",
      "‚Üí Predicted: Mohiri | Target: introductions\n",
      "\n",
      "Context: i leaned out the window and pointed at floyd 's hearing aid . floyd dug the hearing aid out of his ear and made an adjustment . `` sorry , '' he said , screwing it back in . `` blasted thing was turned off . '' `` what happened to chester ? '' `` shot in the head 's what happened to\n",
      "‚Üí Predicted: him | Target: chester\n",
      "\n",
      "Context: ` well i do n't know why , nellie showed it to me many times . it has heaps of her poems and short stories in it ; mostly girlie stuff though , which is why she obviously did n't bother showing you . strange that she left it to you though , i ca n't imagine you would enjoy it very much . ` are you sure it is the same\n",
      "‚Üí Predicted: yes | Target: book\n",
      "\n",
      "Context: `` we 'll wait for you , daddy , '' izzy said , snuggling up to annie . nick pressed the book into izzy 's hands and hurried out of the room . he came back a few minutes later , looking solemn . annie felt a prickling of fear . she sat up straighter , leaning forward . `` nick ? '' he eased back into the bed , on the other side of\n",
      "‚Üí Predicted: truth | Target: izzy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Âä†ËΩΩ base Ê®°Âûã\n",
    "model_base, tokenizer_base = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Phi-4\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=64,\n",
    "    gpu_memory_utilization=0.7,\n",
    "    \n",
    ")\n",
    "FastLanguageModel.for_inference(model_base)\n",
    "evaluate_lambada_structured(model_base, tokenizer_base, dataset, name=\"Base Model (unsloth/Phi-4)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
